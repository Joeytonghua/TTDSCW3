import json
import os
import time
import zlib

import msgpack


class IndexOptimizer:
    """ç”¨äºä¼˜åŒ–å€’æ’ç´¢å¼•çš„å·¥å…·ç±»ï¼Œå‡å°‘å†…å­˜å ç”¨å¹¶æé«˜è®¿é—®é€Ÿåº¦"""

    @staticmethod
    def compress_index(input_file="inverted_index.json", output_file="optimized_index.msgpack"):
        """
        å°†åŸå§‹JSONç´¢å¼•è½¬æ¢ä¸ºä¼˜åŒ–çš„å‹ç¼©æ ¼å¼

        ä¼˜åŒ–ç­–ç•¥ï¼š
        1. ä½¿ç”¨msgpackæ›¿ä»£JSONæé«˜åºåˆ—åŒ–/ååºåˆ—åŒ–é€Ÿåº¦
        2. ä½¿ç”¨å·®åˆ†ç¼–ç å­˜å‚¨ä½ç½®ä¿¡æ¯
        3. ä½¿ç”¨zlibå‹ç¼©æ•´ä½“æ•°æ®
        4. å¯¹æ–‡æ¡£IDä½¿ç”¨æ•´æ•°ç¼–ç 
        """
        print(f"ğŸ“Š å¼€å§‹ä¼˜åŒ–ç´¢å¼•æ–‡ä»¶: {input_file}")
        start_time = time.time()

        # åŠ è½½åŸå§‹ç´¢å¼•
        with open(input_file, "r", encoding="utf-8") as f:
            original_index = json.load(f)

        print(f"ğŸ“ åŸå§‹ç´¢å¼•å¤§å°: {os.path.getsize(input_file) / (1024 * 1024):.2f} MB")
        print(f"ğŸ“ åŸå§‹ç´¢å¼•åŒ…å« {len(original_index)} ä¸ªè¯æ¡")

        # åˆ›å»ºæ–‡æ¡£IDæ˜ å°„è¡¨ï¼ˆå­—ç¬¦ä¸²ID -> æ•´æ•°IDï¼‰
        doc_id_map = {}
        next_doc_id = 0

        # ä¼˜åŒ–åçš„ç´¢å¼•
        optimized_index = {}

        # å¤„ç†æ¯ä¸ªè¯æ¡
        for term, postings in original_index.items():
            term_data = {}

            for doc_id, data in postings.items():
                # ä¸ºå­—ç¬¦ä¸²æ–‡æ¡£IDåˆ†é…æ•´æ•°ID
                if doc_id not in doc_id_map:
                    doc_id_map[doc_id] = next_doc_id
                    next_doc_id += 1

                int_doc_id = doc_id_map[doc_id]

                # å¤„ç†ä½ç½®ä¿¡æ¯ - ä½¿ç”¨å·®åˆ†ç¼–ç 
                positions = data.get("positions", [])
                if positions:
                    # å·®åˆ†ç¼–ç  - å­˜å‚¨ç›¸é‚»ä½ç½®çš„å·®å€¼è€Œä¸æ˜¯ç»å¯¹ä½ç½®
                    diff_positions = []
                    prev_pos = 0
                    for pos in sorted(positions):
                        diff_positions.append(pos - prev_pos)
                        prev_pos = pos

                    term_data[int_doc_id] = diff_positions
                else:
                    term_data[int_doc_id] = []

            optimized_index[term] = term_data

        # åˆ›å»ºå®Œæ•´ä¼˜åŒ–ç´¢å¼•æ•°æ®
        optimized_data = {
            "doc_id_map": doc_id_map,
            "index": optimized_index
        }

        # ä½¿ç”¨msgpackåºåˆ—åŒ–å¹¶å‹ç¼©
        serialized_data = msgpack.packb(optimized_data, use_bin_type=True)
        compressed_data = zlib.compress(serialized_data, level=9)  # æœ€é«˜å‹ç¼©çº§åˆ«

        # ä¿å­˜ä¼˜åŒ–åçš„ç´¢å¼•
        with open(output_file, "wb") as f:
            f.write(compressed_data)

        # è¾“å‡ºä¼˜åŒ–ç»“æœ
        end_time = time.time()
        optimized_size = os.path.getsize(output_file) / (1024 * 1024)
        original_size = os.path.getsize(input_file) / (1024 * 1024)

        print(f"âœ… ç´¢å¼•ä¼˜åŒ–å®Œæˆ!")
        print(f"ğŸ“Š ä¼˜åŒ–åå¤§å°: {optimized_size:.2f} MB")
        print(f"ğŸ“Š å‹ç¼©æ¯”: {original_size / optimized_size:.2f}x")
        print(f"â±ï¸ å¤„ç†æ—¶é—´: {end_time - start_time:.2f} ç§’")

        return {
            "original_size_mb": original_size,
            "optimized_size_mb": optimized_size,
            "compression_ratio": original_size / optimized_size,
            "processing_time_sec": end_time - start_time
        }

    @staticmethod
    def decompress_index(file_path="optimized_index.msgpack"):
        """ä»ä¼˜åŒ–æ ¼å¼åŠ è½½ç´¢å¼•å¹¶è§£å‹"""
        try:
            with open(file_path, "rb") as f:
                compressed_data = f.read()

            # è§£å‹æ•°æ®
            decompressed_data = zlib.decompress(compressed_data)

            # ååºåˆ—åŒ– - æ·»åŠ strict_map_key=Falseå‚æ•°
            optimized_data = msgpack.unpackb(decompressed_data, raw=False, strict_map_key=False)

            return optimized_data
        except Exception as e:
            print(f"âŒ åŠ è½½ä¼˜åŒ–ç´¢å¼•å¤±è´¥: {str(e)}")
            return None


if __name__ == "__main__":
    # æ‰§è¡Œç´¢å¼•ä¼˜åŒ–
    IndexOptimizer.compress_index()