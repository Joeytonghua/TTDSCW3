import sqlite3
import aiohttp
import asyncio
import random
import tqdm  # è¿›åº¦æ¡
import requests
from bs4 import BeautifulSoup
from config import DB_PATH

# é™åˆ¶æœ€å¤§å¹¶å‘è¯·æ±‚æ•°
MAX_CONCURRENT_REQUESTS = 5
TIMEOUT = 15  # è¯·æ±‚è¶…æ—¶æ—¶é—´

# ä¼ªè£…æˆç§»åŠ¨è®¾å¤‡ï¼Œå‡å°‘å°é”
USER_AGENTS = [
    "Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 10) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
]

# å‘é€ Cookie ä»¥è·³è¿‡å¼¹çª—
COOKIES = {
    "euconsent": "true",
    "gdpr": "1",
    "cookie-consent": "accepted",
}


def fetch_with_requests(url):
    """ä½¿ç”¨ requests å¤„ç†è¶…é•¿ HTTP å¤´çš„ç½‘ç«™"""
    try:
        headers = {
            "User-Agent": random.choice(USER_AGENTS),
            "Referer": url,
        }
        response = requests.get(url, headers=headers, cookies=COOKIES, timeout=TIMEOUT, allow_redirects=True)
        if response.status_code != 200:
            print(f"âŒ requests è®¿é—®å¤±è´¥ {url}ï¼ŒçŠ¶æ€ç : {response.status_code}")
            return None
        soup = BeautifulSoup(response.text, "html.parser")
        paragraphs = soup.find_all("p")
        full_text = "\n".join([p.get_text() for p in paragraphs]).strip()
        return full_text if full_text else None
    except Exception as e:
        print(f"âš ï¸ requests è§£æå¤±è´¥ {url}: {e}")
        return None


async def fetch_full_content(session, url, bar):
    """å¼‚æ­¥è·å–å®Œæ•´æ–°é—»æ­£æ–‡"""
    headers = {
        "User-Agent": random.choice(USER_AGENTS),
        "Referer": url,
    }

    try:
        async with session.get(url, headers=headers, cookies=COOKIES, timeout=TIMEOUT,
                               allow_redirects=True) as response:
            if response.status in [403, 404]:
                print(f"ğŸš« è®¿é—®è¢«æ‹’ç»ï¼ˆ{response.status}ï¼‰ï¼Œè·³è¿‡ï¼š{url}")
                bar.update(1)
                return None
            if response.status != 200:
                print(f"âŒ è®¿é—®å¤±è´¥ {url}ï¼ŒçŠ¶æ€ç : {response.status}")
                bar.update(1)
                return None

            html = await response.text(encoding="utf-8", errors="ignore")
            try:
                soup = BeautifulSoup(html, "html.parser")
            except Exception:
                soup = BeautifulSoup(html, "html5lib")

            article = soup.find("article")
            if article:
                paragraphs = article.find_all("p")
            else:
                content_divs = ["article-content", "entry-content", "post-content", "news-content"]
                for div_class in content_divs:
                    article = soup.find("div", class_=div_class)
                    if article:
                        paragraphs = article.find_all("p")
                        break
                else:
                    main = soup.find("main")
                    if main:
                        paragraphs = main.find_all("p")
                    else:
                        bar.update(1)
                        return None

            full_text = "\n".join([p.get_text() for p in paragraphs]).strip()
            bar.update(1)
            return full_text if full_text else None

    except asyncio.TimeoutError:
        print(f"âš ï¸ è¯·æ±‚è¶…æ—¶: {url}")
        bar.update(1)
        return None
    except aiohttp.ClientError as e:
        # å¦‚æœæ˜¯ Header è¿‡é•¿é”™è¯¯ï¼Œç›´æ¥ç”¨ requests å¤„ç†
        if "Got more than" in str(e) and "when reading Header value is too long" in str(e):
            print(f"âš ï¸ Header è¿‡é•¿ï¼Œåˆ‡æ¢ requests å¤„ç†: {url}")
            content = fetch_with_requests(url)
            bar.update(1)
            return content
        print(f"âš ï¸ ç½‘ç»œé”™è¯¯ {url}: {e}")
        bar.update(1)
        return None


async def update_news_content():
    """å¼‚æ­¥æ›´æ–°æ•°æ®åº“ï¼Œå°† content æ›¿æ¢ä¸ºä» URL è§£æçš„å®Œæ•´æ–°é—»æ­£æ–‡"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()

    cursor.execute("SELECT id, url, content FROM news ORDER BY published_at DESC")
    news_items = cursor.fetchall()
    
    failed_news_ids = []  # å­˜å‚¨éœ€è¦åˆ é™¤çš„æ–°é—» ID
    updated_data = []
    sem = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)

    async with aiohttp.ClientSession(connector=aiohttp.TCPConnector(limit=100, ssl=False)) as session:
        with tqdm.tqdm(total=len(news_items), desc="æ­£åœ¨çˆ¬å–æ–°é—»") as bar:
            async def process_news(news_id, url, old_content):
                async with sem:
                    await asyncio.sleep(random.uniform(0.2, 0.8))
                    new_content = await fetch_full_content(session, url, bar)
                    
                    if not new_content or len(new_content) < 50:
                        print(f"âŒ è·å–å¤±è´¥ï¼Œè®°å½•å¾…åˆ é™¤æ–°é—»: {news_id}")
                        failed_news_ids.append(news_id)
                        return  # ç›´æ¥è¿”å›ï¼Œè·³è¿‡å­˜å‚¨
                    
                    updated_data.append((new_content, news_id))

            tasks = [process_news(news_id, url, old_content) for news_id, url, old_content in news_items]
            await asyncio.gather(*tasks)

    # åˆ é™¤è·å–å¤±è´¥çš„æ–°é—»
    if failed_news_ids:
        cursor.executemany("DELETE FROM news WHERE id = ?", [(news_id,) for news_id in failed_news_ids])
        print(f"ğŸ—‘ï¸ å·²åˆ é™¤ {len(failed_news_ids)} æ¡æ— æ•ˆæ–°é—»")

    # æ›´æ–°æ–°é—»å†…å®¹
    if updated_data:
        cursor.executemany("UPDATE news SET content = ? WHERE id = ?", updated_data)
        print(f"ğŸ‰ æ›´æ–°å®Œæˆï¼Œå…±æ›´æ–° {len(updated_data)} æ¡æ–°é—»")

    # é¢å¤–åˆ é™¤ content å•è¯æ•°å°‘äº 15 çš„æ–°é—»
    cursor.execute("DELETE FROM news WHERE (LENGTH(content) - LENGTH(REPLACE(content, ' ', ''))) < 100")
    deleted_rows = cursor.rowcount
    print(f"ğŸ—‘ï¸ é¢å¤–åˆ é™¤ {deleted_rows} æ¡å•è¯æ•°è¿‡å°‘çš„æ–°é—»")
    conn.commit()
    conn.close()


def run_async():
    import sys
    try:
        if sys.platform == "win32":
            asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
        asyncio.run(update_news_content())
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ·ç»ˆæ­¢ï¼Œå®‰å…¨é€€å‡º...")


if __name__ == "__main__":
    run_async()
