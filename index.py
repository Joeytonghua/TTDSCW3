# import sqlite3
# import re
# import nltk
# import json
# from nltk.corpus import stopwords
# from nltk.stem import PorterStemmer
# from collections import defaultdict
# from config import DB_PATH
# from tqdm import tqdm  # å¯¼å…¥ tqdm åº“

# # ç¡®ä¿åœç”¨è¯åº“å·²ä¸‹è½½
# # nltk.download("stopwords")

# class Indexer:
#     def __init__(self):
#         self.inverted_index = defaultdict(dict)  # å€’æ’ç´¢å¼•ç»“æ„
#         self.doc_lengths = {}  # è®°å½•æ¯ä¸ªæ–‡æ¡£çš„é•¿åº¦
#         self.stop_words = set(stopwords.words("english"))  # åœç”¨è¯è¡¨
#         self.stemmer = PorterStemmer()  # è¯å¹²æå–å™¨

#     def preprocess_text(self, text):
#         """æ¸…ç†æ–‡æœ¬ï¼šå°å†™è½¬æ¢ã€å»é™¤æ ‡ç‚¹ã€å»åœç”¨è¯ã€è¯å¹²åŒ–"""
#         text = text.lower()
#         text = re.sub(r"[^\w\s]", "", text)
#         words = text.split()
#         processed_tokens = [self.stemmer.stem(word) for word in words if word not in self.stop_words]
#         return processed_tokens

#     def fetch_news_data(self):
#         """ä» SQLite æ•°æ®åº“æå–æ–°é—»æ•°æ®"""
#         conn = sqlite3.connect(DB_PATH)
#         cursor = conn.cursor()
#         cursor.execute("SELECT id, title, content FROM news")
#         documents = cursor.fetchall()
#         conn.close()
#         return documents

#     def build_inverted_index(self):
#         """æ„å»ºå€’æ’ç´¢å¼•"""
#         documents = self.fetch_news_data()
#         total_docs = len(documents)
#         total_length = 0

#         for doc_id, title, content in tqdm(documents, desc="æ„å»ºå€’æ’ç´¢å¼•"):
#             combined_text = f"{title} {content}"  # åˆå¹¶ title + content
#             tokens = self.preprocess_text(combined_text)
#             self.doc_lengths[doc_id] = len(tokens)
#             total_length += len(tokens)

#             for pos, token in enumerate(tokens):
#                 if doc_id not in self.inverted_index[token]:
#                     self.inverted_index[token][doc_id] = []
#                 self.inverted_index[token][doc_id].append(pos)

#         self.avg_doc_length = total_length / total_docs if total_docs else 1
#         return total_docs

#     def save_index(self):
#         """å­˜å‚¨ç´¢å¼•åˆ° JSON æ–‡ä»¶"""
#         with open("inverted_index.json", "w", encoding="utf-8") as f:
#             json.dump(self.inverted_index, f, indent=4)
#         print("âœ… å€’æ’ç´¢å¼•å·²ä¿å­˜è‡³ inverted_index.json")

#     def build_and_store_index(self):
#         """æ„å»ºç´¢å¼•å¹¶å­˜å‚¨"""
#         print("ğŸ“Œ å¼€å§‹æ„å»ºå€’æ’ç´¢å¼•...")
#         self.build_inverted_index()
#         self.save_index()
#         print("ğŸ‰ ç´¢å¼•æ„å»ºå®Œæˆï¼")


# if __name__ == "__main__":
#     indexer = Indexer()
#     indexer.build_and_store_index()


import sqlite3
import re
import nltk
import math
import json
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from collections import defaultdict
from config import DB_PATH


# ç¡®ä¿åœç”¨è¯åº“å·²ä¸‹è½½
# nltk.download("stopwords")

class Indexer:
    def __init__(self):
        self.inverted_index = defaultdict(dict)  # å€’æ’ç´¢å¼•ç»“æ„
        self.doc_lengths = {}  # è®°å½•æ¯ä¸ªæ–‡æ¡£çš„é•¿åº¦
        self.stop_words = set(stopwords.words("english"))  # åœç”¨è¯è¡¨
        self.stemmer = PorterStemmer()  # è¯å¹²æå–å™¨

    def preprocess_text(self, text):
        """æ¸…ç†æ–‡æœ¬ï¼šå°å†™è½¬æ¢ã€å»é™¤æ ‡ç‚¹ã€å»åœç”¨è¯ã€è¯å¹²åŒ–"""
        text = text.lower()
        text = re.sub(r"[^\w\s]", "", text)
        words = text.split()
        processed_tokens = [self.stemmer.stem(word) for word in words if word not in self.stop_words]
        return processed_tokens

    def fetch_news_data(self):
        """ä» SQLite æ•°æ®åº“æå–æ–°é—»æ•°æ®"""
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        cursor.execute("SELECT id, title, content FROM news")
        documents = cursor.fetchall()
        conn.close()
        return documents

    def build_inverted_index(self):
        """æ„å»ºå€’æ’ç´¢å¼•"""
        documents = self.fetch_news_data()
        total_docs = len(documents)
        total_length = 0

        for doc_id, title, content in documents:
            combined_text = f"{title} {content}"  # åˆå¹¶ title + content
            tokens = self.preprocess_text(combined_text)
            self.doc_lengths[doc_id] = len(tokens)
            total_length += len(tokens)

            for pos, token in enumerate(tokens):
                if doc_id not in self.inverted_index[token]:
                    self.inverted_index[token][doc_id] = []
                self.inverted_index[token][doc_id].append(pos)

        self.avg_doc_length = total_length / total_docs if total_docs else 1
        return total_docs

    def compute_tf_idf_and_bm25(self, total_docs, k1=1.5, b=0.75):
        """è®¡ç®— TF-IDF å’Œ BM25 è¯„åˆ†"""
        index_data = {}
        for term, doc_dict in self.inverted_index.items():
            # df = len(doc_dict)
            # idf = math.log((total_docs + 1) / (df + 1)) + 1  # è®¡ç®— IDF

            for doc_id, positions in doc_dict.items():
                # tf = len(positions)
                # tf_idf_score = tf * idf

                # # è®¡ç®— BM25 è¯„åˆ†
                # doc_length = self.doc_lengths.get(doc_id, 0)
                # bm25_score = idf * ((tf * (k1 + 1)) / (tf + k1 * (1 - b + b * (doc_length / self.avg_doc_length))))

                if term not in index_data:
                    index_data[term] = {}
                index_data[term][doc_id] = {
                    # "tf-idf": tf_idf_score,
                    # "bm25": bm25_score,
                    "positions": positions
                }
        return index_data

    def save_index(self, index_data):
        """å­˜å‚¨ç´¢å¼•åˆ° JSON æ–‡ä»¶"""
        with open("inverted_index.json", "w", encoding="utf-8") as f:
            json.dump(index_data, f, indent=4)
        print("âœ… å€’æ’ç´¢å¼•å·²ä¿å­˜è‡³ inverted_index.json")

    def build_and_store_index(self):
        """æ„å»ºç´¢å¼•å¹¶å­˜å‚¨"""
        print("ğŸ“Œ å¼€å§‹æ„å»ºå€’æ’ç´¢å¼•...")
        total_docs = self.build_inverted_index()
        print("ğŸ“Œ è®¡ç®— TF-IDF å’Œ BM25 è¯„åˆ†...")
        index_data = self.compute_tf_idf_and_bm25(total_docs)
        self.save_index(index_data)
        print("ğŸ‰ ç´¢å¼•æ„å»ºå®Œæˆï¼")


if __name__ == "__main__":
    indexer = Indexer()
    indexer.build_and_store_index()
