import aiohttp
import asyncio
import datetime
import hashlib
import json
from queries import QUERIES  # ä»ç‹¬ç«‹çš„ queries.py æ–‡ä»¶å¯¼å…¥
from config import GNEWS_API_KEY, GNEWS_BASE_URL
from database import insert_news, check_duplicate  # check_duplicate ç”¨äºå»é‡

# API é™åˆ¶ï¼šæ¯å¤© 25,000 æ¡
TARGET_COUNT = 10000000
BATCH_SIZE = 100  # API æ¯æ¬¡è¿”å› 100 æ¡
START_YEAR = 2009  # ä» 2009 å¹´æŠ“å–
END_YEAR = 2025  # ç›´åˆ° 2025 å¹´

async def fetch_news(session, query, from_date, to_date):
    """ä» GNews API å¼‚æ­¥è·å–æ–°é—»"""
    url = (f"{GNEWS_BASE_URL}q={query}&lang=en&max={BATCH_SIZE}&apikey={GNEWS_API_KEY}"
           f"&from={from_date}&to={to_date}")
    
    async with session.get(url) as response:
        if response.status != 200:
            print(f"âŒ è¯·æ±‚å¤±è´¥: {await response.text()}")
            return []
        
        data = await response.json()
        return data.get("articles", [])

async def save_gnews():
    """éå† 2009-2025 å¹´ï¼Œæ¯ä¸ªç±»åˆ«ï¼Œæ¯ä¸ªå…³é”®è¯ï¼ŒæŠ“å–æ–°é—»"""
    total_saved = 0
    async with aiohttp.ClientSession() as session:
        for year in range(START_YEAR, END_YEAR + 1):
            for category, keywords in QUERIES.items():
                for query in keywords:
                    if total_saved >= TARGET_COUNT:
                        break

                    # è®¾ç½®æ—¶é—´èŒƒå›´
                    from_date = f"{year}-01-01T00:00:00Z"
                    to_date = f"{year}-12-31T23:59:59Z"

                    print(f"ğŸ“¢ æ­£åœ¨æŠ“å–: {query} ({category}) {year}")
                    articles = await fetch_news(session, query, from_date, to_date)

                    for article in articles:
                        if total_saved >= TARGET_COUNT:
                            break

                        title = article.get("title", "No Title")
                        description = article.get("description", "No Description")
                        content = article.get("content", "No Content")
                        url = article.get("url", "No URL")
                        published_at = article.get("publishedAt", "No Date")
                        source_name = article.get("source", {}).get("name", "Unknown Source")
                        source_url = article.get("source", {}).get("url", "No Source URL")

                        # è®¡ç®— hashï¼ˆå»é‡ï¼‰
                        news_hash = hashlib.md5(f"{title}{published_at}".encode()).hexdigest()
                        if check_duplicate(news_hash):
                            continue  # è·³è¿‡é‡å¤æ•°æ®

                        if content:
                            insert_news(news_hash, title, description, content, url, published_at, source_name, source_url)
                            total_saved += 1

                    print(f"ğŸ“Š å½“å‰å·²å­˜å…¥: {total_saved}/{TARGET_COUNT}")
                    await asyncio.sleep(1)  # é¿å… API é™åˆ¶

    print(f"âœ… æ€»å…±å­˜å…¥ {total_saved} ç¯‡æ–°é—»")

if __name__ == "__main__":
    asyncio.run(save_gnews())
